{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from istatapi.base import ISTAT\n",
    "from istatapi.utils import make_tree, strip_ns\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovery\n",
    "\n",
    "> Functions used to discover and explore the data exposed by ISTAT webservice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module implements functions to discover the data exposed by ISTAT. To do so, `istatapi` make metadata requests to the API endpoints. The `Discovery` module provides useful methods to parse and analyze API metadata responses. It makes use of the library `pandas` and returns data in the `DataFrame` format, making it convenient for interactive and exploratory analysis in Jupyter Notebooks.\n",
    "\n",
    "The main class implemented in the `Discovery` module is `DataSet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def parse_dataflows(response):\n",
    "    \"\"\"parse the `response` containing all the available datasets and return a list of dataflows.\"\"\"\n",
    "    tree = make_tree(response)\n",
    "    strip_ns(tree)\n",
    "    root = tree.root\n",
    "\n",
    "    dataflows_l = []\n",
    "    for dataflow in root.iter(\"Dataflow\"):\n",
    "        id = dataflow.get(\"id\")\n",
    "        version = dataflow.get(\"version\")\n",
    "        structure_id = [ref.get(\"id\") for ref in dataflow.iter(\"Ref\")][0]\n",
    "\n",
    "        # iter over names and get the descriptions\n",
    "        for name in dataflow.findall(\"Name\"):\n",
    "            lang = name.get(\"{http://www.w3.org/XML/1998/namespace}lang\")\n",
    "            if lang == \"en\":\n",
    "                description_en = name.text\n",
    "            # if lang == 'it':\n",
    "            # description_it = name.text\n",
    "\n",
    "        dataflow_dict = {\n",
    "            \"df_id\": id,\n",
    "            \"version\": version,\n",
    "            \"df_description\": description_en,\n",
    "            # \"description_it\": description_it,\n",
    "            \"df_structure_id\": structure_id,\n",
    "        }\n",
    "\n",
    "        dataflows_l.append(dataflow_dict)\n",
    "\n",
    "    return dataflows_l\n",
    "\n",
    "\n",
    "def all_available(dataframe=True):\n",
    "    \"\"\"Return all available dataflows\"\"\"\n",
    "    path = \"dataflow/IT1\"\n",
    "    client = ISTAT()\n",
    "    response = client._request(path=path)\n",
    "    dataflows = parse_dataflows(response)\n",
    "\n",
    "    if dataframe == True:\n",
    "        dataflows = pd.DataFrame(dataflows)\n",
    "\n",
    "    return dataflows\n",
    "\n",
    "\n",
    "def search_dataset(keyword):\n",
    "    \"\"\"Search available dataflows that contain `keyword`. Return these dataflows in a DataFrame\"\"\"\n",
    "    dataflows = all_available()[\n",
    "        all_available()[\"df_description\"].str.contains(keyword, case=False)\n",
    "    ]\n",
    "    \n",
    "    if len(dataflows) == 0: raise ValueError('No dataset matching `keyword`')\n",
    "\n",
    "    return dataflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to get a full list of the dataflows provided by ISTAT is to call the method `all_available()` which returns a list of all the explorable dataflows, together with their IDs and descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Attol8/istatapi/blob/master/istatapi/discovery.py#L45){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### all_available\n",
       "\n",
       ">      all_available (dataframe=True)\n",
       "\n",
       "Return all available dataflows"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Attol8/istatapi/blob/master/istatapi/discovery.py#L45){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### all_available\n",
       "\n",
       ">      all_available (dataframe=True)\n",
       "\n",
       "Return all available dataflows"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(all_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>df_id</th>\n",
       "      <th>version</th>\n",
       "      <th>df_description</th>\n",
       "      <th>df_structure_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101_1015</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Crops</td>\n",
       "      <td>DCSP_COLTIVAZIONI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101_1030</td>\n",
       "      <td>1.0</td>\n",
       "      <td>PDO, PGI and TSG quality products</td>\n",
       "      <td>DCSP_DOPIGP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101_1033</td>\n",
       "      <td>1.0</td>\n",
       "      <td>slaughtering</td>\n",
       "      <td>DCSP_MACELLAZIONI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101_1039</td>\n",
       "      <td>1.2</td>\n",
       "      <td>Agritourism - municipalities</td>\n",
       "      <td>DCSP_AGRITURISMO_COM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101_1077</td>\n",
       "      <td>1.0</td>\n",
       "      <td>PDO, PGI and TSG products:  operators - munici...</td>\n",
       "      <td>DCSP_DOPIGP_COM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      df_id version                                     df_description  \\\n",
       "0  101_1015     1.3                                              Crops   \n",
       "1  101_1030     1.0                  PDO, PGI and TSG quality products   \n",
       "2  101_1033     1.0                                       slaughtering   \n",
       "3  101_1039     1.2                       Agritourism - municipalities   \n",
       "4  101_1077     1.0  PDO, PGI and TSG products:  operators - munici...   \n",
       "\n",
       "        df_structure_id  \n",
       "0     DCSP_COLTIVAZIONI  \n",
       "1           DCSP_DOPIGP  \n",
       "2     DCSP_MACELLAZIONI  \n",
       "3  DCSP_AGRITURISMO_COM  \n",
       "4       DCSP_DOPIGP_COM  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_datasets = all_available()\n",
    "available_datasets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of available datasets: 509\n"
     ]
    }
   ],
   "source": [
    "print(f'number of available datasets: {len(available_datasets)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(available_datasets.columns, ['df_id', 'version', 'df_description', 'df_structure_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Attol8/istatapi/blob/master/istatapi/discovery.py#L58){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### search_dataset\n",
       "\n",
       ">      search_dataset (keyword)\n",
       "\n",
       "Search available dataflows that contain `keyword`. Return these dataflows in a DataFrame"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Attol8/istatapi/blob/master/istatapi/discovery.py#L58){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### search_dataset\n",
       "\n",
       ">      search_dataset (keyword)\n",
       "\n",
       "Search available dataflows that contain `keyword`. Return these dataflows in a DataFrame"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(search_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method looks for `keyword` inside all datasets descriptions. By default, the `keyword` needs to be an english word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>df_id</th>\n",
       "      <th>version</th>\n",
       "      <th>df_description</th>\n",
       "      <th>df_structure_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>168_261</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Hicp - at constant tax rates annual data(base ...</td>\n",
       "      <td>DCSP_IPCATC2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>168_306</td>\n",
       "      <td>1.2</td>\n",
       "      <td>Hicp - at constant tax rates monthly data (bas...</td>\n",
       "      <td>DCSP_IPCATC1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>168_756</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Hicp - at constant tax rates monthly data (bas...</td>\n",
       "      <td>DCSP_IPCATC1B2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>168_757</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Hicp- at constant tax rates annual data (base ...</td>\n",
       "      <td>DCSP_IPCATC2B2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>30_1008</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Irpef taxable incomes (Ipef) - municipalities</td>\n",
       "      <td>MEF_REDDITIIRPEF_COM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       df_id version                                     df_description  \\\n",
       "168  168_261     1.1  Hicp - at constant tax rates annual data(base ...   \n",
       "169  168_306     1.2  Hicp - at constant tax rates monthly data (bas...   \n",
       "172  168_756     1.4  Hicp - at constant tax rates monthly data (bas...   \n",
       "173  168_757     1.1  Hicp- at constant tax rates annual data (base ...   \n",
       "267  30_1008     1.1      Irpef taxable incomes (Ipef) - municipalities   \n",
       "\n",
       "          df_structure_id  \n",
       "168          DCSP_IPCATC2  \n",
       "169          DCSP_IPCATC1  \n",
       "172     DCSP_IPCATC1B2015  \n",
       "173     DCSP_IPCATC2B2015  \n",
       "267  MEF_REDDITIIRPEF_COM  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = search_dataset(keyword=\"Tax\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fail(lambda: search_dataset(keyword=\"disoccupazione\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structures and Information about available Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<<<<<<< HEAD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# TODO: Add datasets description attribute\n",
    "\n",
    "@dataclass\n",
    "class DataSet():\n",
    "    \"\"\"Class that implements methods to retrieve informations (metadata) about a Dataset\"\"\"\n",
    "    dataflow_identifier: str\n",
    "    all_available: pd.DataFrame = available_datasets\n",
    "    resource: str = \"datastructure\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.identifiers = self.set_identifiers(self.dataflow_identifier)\n",
    "        self.dimensions = self.dimensions_info(description=False).dimension.to_list()\n",
    "        self.filters = self.default_filters()\n",
    "        self.available_values = self.get_available_values()\n",
    "\n",
    "    def set_identifiers(self, dataflow_identifier):\n",
    "        \"\"\"Take any type of `dataflow_identifier` and return all identifiers in a dictionary\"\"\"\n",
    "        if dataflow_identifier[1] == \"_\" or dataflow_identifier[2] == \"_\" or dataflow_identifier[3] == \"_\":\n",
    "            return self.set_from_id(dataflow_identifier)\n",
    "        elif dataflow_identifier[4] == \"_\":\n",
    "            return self.set_from_structure_id(dataflow_identifier)\n",
    "        else:\n",
    "            if type(dataflow_identifier) == str:\n",
    "                return self.set_from_description(dataflow_identifier)\n",
    "            else:\n",
    "                raise ValueError(dataflow_identifier)\n",
    "\n",
    "    def set_from_id(self, df_id):\n",
    "        mask = self.all_available[\"df_id\"] == df_id\n",
    "        df = self.all_available[mask]\n",
    "        return df.to_dict(orient=\"records\")[0]\n",
    "\n",
    "    def set_from_structure_id(self, df_structure_id):\n",
    "        mask = self.all_available[\"df_structure_id\"] == df_structure_id\n",
    "        df = self.all_available[mask]\n",
    "        return df.to_dict(orient=\"records\")[0]\n",
    "\n",
    "    def set_from_description(self, description):\n",
    "        mask = self.all_available[\"df_description\"] == description\n",
    "        df = self.all_available[mask]\n",
    "        return df.to_dict(orient=\"records\")[0]\n",
    "\n",
    "    def parse_dimensions(self, response):\n",
    "        \"\"\"Parse the `response` containing a dataflow's dimensions and return them in a list\"\"\"\n",
    "        tree = make_tree(response)\n",
    "        strip_ns(tree)\n",
    "        root = tree.root\n",
    "\n",
    "        dimensions_l = []\n",
    "        for dimension in root.iter(\"Dimension\"):\n",
    "            dimension_name = dimension.attrib[\"id\"]\n",
    "\n",
    "            dimension_id = [\n",
    "                enumeration.find(\"Ref\").get(\"id\")\n",
    "                for enumeration in dimension.iter(\"Enumeration\")\n",
    "            ][0]\n",
    "\n",
    "            dimension_dict = {\"dimension\": dimension_name, \"dimension_ID\": dimension_id}\n",
    "\n",
    "            dimensions_l.append(dimension_dict)\n",
    "\n",
    "        return dimensions_l\n",
    "\n",
    "    def dimensions_info(self, dataframe=True, description=True):\n",
    "        \"\"\"Return the dimensions of a specific dataflow and their `descriptions`.\"\"\"\n",
    "        df_structure_id = self.identifiers[\"df_structure_id\"]\n",
    "        client = ISTAT()\n",
    "        path_parts = [self.resource, client.agencyID, df_structure_id]\n",
    "        path = \"/\".join(path_parts)\n",
    "        response = client._request(path=path)\n",
    "        dimensions = self.parse_dimensions(response)\n",
    "\n",
    "        if dataframe == True:\n",
    "            dimensions = pd.DataFrame(dimensions)\n",
    "\n",
    "        if description == True:\n",
    "            dimensions_description = self.dimensions_description(dimensions)\n",
    "            dimensions = dimensions.merge(dimensions_description, on=\"dimension_ID\")\n",
    "\n",
    "        return dimensions\n",
    "\n",
    "    def dimensions_description(self, dimensions):\n",
    "        \"\"\"Return a dataframe with the descriptions of `dimensions`\"\"\"\n",
    "        resource = \"codelist\"\n",
    "        dimensions_l = dimensions.dimension_ID.tolist()\n",
    "        descriptions_l = []\n",
    "\n",
    "        for dimension_id in dimensions_l:\n",
    "            client = ISTAT()\n",
    "            path_parts = [resource, client.agencyID, dimension_id]\n",
    "            path = \"/\".join(path_parts)\n",
    "            response = client._request(path=path)\n",
    "            tree = make_tree(response)\n",
    "            strip_ns(tree)\n",
    "            root = tree.root\n",
    "\n",
    "            description = [x for x in root.iter(\"Codelist\")][0]\n",
    "            # description_it = description.findall('Name')[0].text\n",
    "            description = description.findall(\"Name\")[1].text\n",
    "\n",
    "            description_dict = {\n",
    "                \"dimension_ID\": dimension_id,\n",
    "                \"description\": description,\n",
    "            }\n",
    "            descriptions_l.append(description_dict)\n",
    "\n",
    "        dimensions_descriptions = pd.DataFrame(descriptions_l)\n",
    "\n",
    "        return dimensions_descriptions\n",
    "\n",
    "    def get_available_values(self):\n",
    "        \"\"\"Return a dictionary with available values for each dimension in the DataSet instance\"\"\"\n",
    "        resource = \"availableconstraint\"\n",
    "        client = ISTAT()\n",
    "        df_id = self.identifiers[\"df_id\"]\n",
    "        path_parts = [\n",
    "            resource,\n",
    "            df_id,\n",
    "            \"?references=all&detail=full\",\n",
    "        ]  # TODO: pass them as parameters\n",
    "        path = \"/\".join(path_parts)\n",
    "        response = client._request(path=path)\n",
    "        tree = make_tree(response)\n",
    "        strip_ns(tree)\n",
    "        root = tree.root\n",
    "\n",
    "        dimensions_values = {}\n",
    "\n",
    "        for dimension in root.iter(\"Codelist\"):\n",
    "            dimension_id = dimension.get(\"id\")\n",
    "\n",
    "            values = {}\n",
    "            value_id_l, value_descr_l = [], []\n",
    "\n",
    "            for value in dimension.iter(\"Code\"):\n",
    "                value_id = value.get(\"id\")\n",
    "                value_descr = [name.text for name in value.findall(\"Name\")][1]\n",
    "                value_id_l.append(value_id)\n",
    "                value_descr_l.append(value_descr)\n",
    "\n",
    "            values[\"values_ids\"] = value_id_l\n",
    "            values[\"values_description\"] = value_descr_l\n",
    "            dimensions_values[dimension_id] = values\n",
    "\n",
    "        for dimension_id in list(dimensions_values.keys()):\n",
    "            dimension = self.get_dimension_name(dimension_id)\n",
    "            dimensions_values[dimension] = dimensions_values.pop(dimension_id)\n",
    "\n",
    "        return dimensions_values\n",
    "\n",
    "    def get_dimension_values(self, dimension, dataframe=True):\n",
    "        \"\"\"Return the available values of a single `dimension` in the dataset\"\"\"\n",
    "        dimension_dict = self.available_values[dimension]\n",
    "        dimension_df = pd.DataFrame.from_dict(dimension_dict)\n",
    "        return dimension_df if dataframe else dimension_dict\n",
    "\n",
    "    def get_dimension_name(self, dimension_id):\n",
    "        \"\"\"Convert `dimension_id` to `dimension`\"\"\"\n",
    "        dimensions_df = self.dimensions_info(description=False)\n",
    "        mask = dimensions_df[\"dimension_ID\"] == dimension_id\n",
    "        dimension = dimensions_df[mask][\"dimension\"]\n",
    "        return dimension.values[0]\n",
    "\n",
    "    def default_filters(self):\n",
    "        \"\"\"\"initiate self.filters with default values\"\"\"\n",
    "        default_filters = {}  \n",
    "        # no filter equals all values (default)\n",
    "        for dimension in self.dimensions:\n",
    "            default_filters[dimension] = \".\"\n",
    "        return default_filters\n",
    "\n",
    "    def set_filters(self, **kwargs):\n",
    "        \"\"\"set filters for the dimensions of the dataset by passing dimension_name=value\"\"\"\n",
    "        # add kwargs in case passed\n",
    "        for arg, arg_value in kwargs.items():\n",
    "            self.filters[arg.upper()] = arg_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`=======`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# TODO: Add datasets description attribute\n",
    "\n",
    "class DataSet(ISTAT):\n",
    "    \"\"\"dataclass that implements methods to retrieve informations (metadata) about a Dataset and store them as attributes.\"\"\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.identifiers = self.set_identifiers(self.dataflow_identifier)\n",
    "        self.dimensions = self.dimensions_info(description=False).dimension.to_list()\n",
    "        self.filters = self.default_filters()\n",
    "        self.available_values = self.get_available_values()\n",
    "\n",
    "    def set_identifiers(self, dataflow_identifier):\n",
    "        \"\"\"Take any type of `dataflow_identifier` and return all identifiers in a dictionary\"\"\"\n",
    "        if dataflow_identifier[1] == \"_\" or dataflow_identifier[2] == \"_\" or dataflow_identifier[3] == \"_\":\n",
    "            return self.set_from_id(dataflow_identifier)\n",
    "        elif dataflow_identifier[4] == \"_\":\n",
    "            return self.set_from_structure_id(dataflow_identifier)\n",
    "        else:\n",
    "            if type(dataflow_identifier) == str:\n",
    "                return self.set_from_description(dataflow_identifier)\n",
    "            else:\n",
    "                raise ValueError(dataflow_identifier)\n",
    "\n",
    "    def set_from_id(self, df_id):\n",
    "        mask = self.all_available[\"df_id\"] == df_id\n",
    "        df = self.all_available[mask]\n",
    "        return df.to_dict(orient=\"records\")[0]\n",
    "\n",
    "    def set_from_structure_id(self, df_structure_id):\n",
    "        mask = self.all_available[\"df_structure_id\"] == df_structure_id\n",
    "        df = self.all_available[mask]\n",
    "        return df.to_dict(orient=\"records\")[0]\n",
    "\n",
    "    def set_from_description(self, description):\n",
    "        mask = self.all_available[\"df_description\"] == description\n",
    "        df = self.all_available[mask]\n",
    "        return df.to_dict(orient=\"records\")[0]\n",
    "\n",
    "    def parse_dimensions(self, response):\n",
    "        \"\"\"Parse the `response` containing a dataflow's dimensions and return them in a list\"\"\"\n",
    "        tree = make_tree(response)\n",
    "        strip_ns(tree)\n",
    "        root = tree.root\n",
    "\n",
    "        dimensions_l = []\n",
    "        for dimension in root.iter(\"Dimension\"):\n",
    "            dimension_name = dimension.attrib[\"id\"]\n",
    "\n",
    "            dimension_id = [\n",
    "                enumeration.find(\"Ref\").get(\"id\")\n",
    "                for enumeration in dimension.iter(\"Enumeration\")\n",
    "            ][0]\n",
    "\n",
    "            dimension_dict = {\"dimension\": dimension_name, \"dimension_ID\": dimension_id}\n",
    "\n",
    "            dimensions_l.append(dimension_dict)\n",
    "\n",
    "        return dimensions_l\n",
    "\n",
    "    def dimensions_info(self, dataframe=True, description=True):\n",
    "        \"\"\"Return the dimensions of a specific dataflow and their `descriptions`.\"\"\"\n",
    "        df_structure_id = self.identifiers[\"df_structure_id\"]\n",
    "        client = ISTAT()\n",
    "        path_parts = [self.resource, client.agencyID, df_structure_id]\n",
    "        path = \"/\".join(path_parts)\n",
    "        response = client._request(path=path)\n",
    "        dimensions = self.parse_dimensions(response)\n",
    "\n",
    "        if dataframe == True:\n",
    "            dimensions = pd.DataFrame(dimensions)\n",
    "\n",
    "        if description == True:\n",
    "            dimensions_description = self.dimensions_description(dimensions)\n",
    "            dimensions = dimensions.merge(dimensions_description, on=\"dimension_ID\")\n",
    "\n",
    "        return dimensions\n",
    "\n",
    "    def dimensions_description(self, dimensions):\n",
    "        \"\"\"Return a dataframe with the descriptions of `dimensions`\"\"\"\n",
    "        resource = \"codelist\"\n",
    "        dimensions_l = dimensions.dimension_ID.tolist()\n",
    "        descriptions_l = []\n",
    "\n",
    "        for dimension_id in dimensions_l:\n",
    "            client = ISTAT()\n",
    "            path_parts = [resource, client.agencyID, dimension_id]\n",
    "            path = \"/\".join(path_parts)\n",
    "            response = client._request(path=path)\n",
    "            tree = make_tree(response)\n",
    "            strip_ns(tree)\n",
    "            root = tree.root\n",
    "\n",
    "            description = [x for x in root.iter(\"Codelist\")][0]\n",
    "            # description_it = description.findall('Name')[0].text\n",
    "            description = description.findall(\"Name\")[1].text\n",
    "\n",
    "            description_dict = {\n",
    "                \"dimension_ID\": dimension_id,\n",
    "                \"description\": description,\n",
    "            }\n",
    "            descriptions_l.append(description_dict)\n",
    "\n",
    "        dimensions_descriptions = pd.DataFrame(descriptions_l)\n",
    "\n",
    "        return dimensions_descriptions\n",
    "\n",
    "    def get_available_values(self):\n",
    "        \"\"\"Return a dictionary with available values for each dimension in the DataSet instance\"\"\"\n",
    "        resource = \"availableconstraint\"\n",
    "        client = ISTAT()\n",
    "        df_id = self.identifiers[\"df_id\"]\n",
    "        path_parts = [\n",
    "            resource,\n",
    "            df_id,\n",
    "            \"?references=all&detail=full\",\n",
    "        ]  # TODO: pass them as parameters\n",
    "        path = \"/\".join(path_parts)\n",
    "        response = client._request(path=path)\n",
    "        tree = make_tree(response)\n",
    "        strip_ns(tree)\n",
    "        root = tree.root\n",
    "\n",
    "        dimensions_values = {}\n",
    "\n",
    "        for dimension in root.iter(\"Codelist\"):\n",
    "            dimension_id = dimension.get(\"id\")\n",
    "\n",
    "            values = {}\n",
    "            value_id_l, value_descr_l = [], []\n",
    "\n",
    "            for value in dimension.iter(\"Code\"):\n",
    "                value_id = value.get(\"id\")\n",
    "                value_descr = [name.text for name in value.findall(\"Name\")][1]\n",
    "                value_id_l.append(value_id)\n",
    "                value_descr_l.append(value_descr)\n",
    "\n",
    "            values[\"values_ids\"] = value_id_l\n",
    "            values[\"values_description\"] = value_descr_l\n",
    "            dimensions_values[dimension_id] = values\n",
    "\n",
    "        for dimension_id in list(dimensions_values.keys()):\n",
    "            dimension = self.get_dimension_name(dimension_id)\n",
    "            dimensions_values[dimension] = dimensions_values.pop(dimension_id)\n",
    "\n",
    "        return dimensions_values\n",
    "\n",
    "    def get_dimension_values(self, dimension, dataframe=True):\n",
    "        \"\"\"Return the available values of a single `dimension` in the dataset\"\"\"\n",
    "        dimension_dict = self.available_values[dimension]\n",
    "        dimension_df = pd.DataFrame.from_dict(dimension_dict)\n",
    "        return dimension_df if dataframe else dimension_dict\n",
    "\n",
    "    def get_dimension_name(self, dimension_id):\n",
    "        \"\"\"Convert `dimension_id` to `dimension`\"\"\"\n",
    "        dimensions_df = self.dimensions_info(description=False)\n",
    "        mask = dimensions_df[\"dimension_ID\"] == dimension_id\n",
    "        dimension = dimensions_df[mask][\"dimension\"]\n",
    "        return dimension.values[0]\n",
    "\n",
    "    def default_filters(self):\n",
    "        \"\"\"\"initiate self.filters with default values\"\"\"\n",
    "        default_filters = {}  \n",
    "        # no filter equals all values (default)\n",
    "        for dimension in self.dimensions:\n",
    "            default_filters[dimension] = \".\"\n",
    "        return default_filters\n",
    "\n",
    "    def set_filters(self, **kwargs):\n",
    "        \"\"\"set filters for the dimensions of the dataset by passing dimension_name=value\"\"\"\n",
    "        # add kwargs in case passed\n",
    "        for arg, arg_value in kwargs.items():\n",
    "            self.filters[arg.upper()] = arg_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`>>>>>>> master`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class takes `df_id`, `df_structure_id` or `df_description` as inputs. These 3 values can be found by using the `all_available()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ds \u001b[39m=\u001b[39m DataSet(dataflow_identifier\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m151_914\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m test_eq(ds\u001b[39m.\u001b[39midentifiers[\u001b[39m'\u001b[39m\u001b[39mdf_id\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m'\u001b[39m\u001b[39m151_914\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m test_eq(ds\u001b[39m.\u001b[39midentifiers[\u001b[39m'\u001b[39m\u001b[39mdf_description\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mUnemployment  rate\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[26], line 14\u001b[0m, in \u001b[0;36mDataSet.__init__\u001b[0;34m(self, dataflow_identifier)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39midentifiers \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_identifiers(dataflow_identifier)\n\u001b[1;32m     13\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavailable_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_available_values()\n\u001b[0;32m---> 14\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdimensions \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdimensions_info(description\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\u001b[39m.\u001b[39mdimension)\n\u001b[1;32m     15\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault_filters()\n",
      "Cell \u001b[0;32mIn[26], line 73\u001b[0m, in \u001b[0;36mDataSet.dimensions_info\u001b[0;34m(self, dataframe, description)\u001b[0m\n\u001b[1;32m     71\u001b[0m path_parts \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresource, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magencyID, df_structure_id]\n\u001b[1;32m     72\u001b[0m path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(path_parts)\n\u001b[0;32m---> 73\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(path\u001b[39m=\u001b[39;49mpath)\n\u001b[1;32m     74\u001b[0m dimensions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_dimensions(response)\n\u001b[1;32m     76\u001b[0m \u001b[39mif\u001b[39;00m dataframe \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/c/Users/44749/OneDrive/Desktop/projects/istatapi/istatapi/base.py:25\u001b[0m, in \u001b[0;36mISTAT._request\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_url, path])\n\u001b[1;32m     24\u001b[0m headers[\u001b[39m\"\u001b[39m\u001b[39mUser-Agent\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 25\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(url, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[1;32m     27\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/istatapi/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/istatapi/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/istatapi/lib/python3.10/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/istatapi/lib/python3.10/site-packages/requests/sessions.py:723\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[39mif\u001b[39;00m allow_redirects:\n\u001b[1;32m    721\u001b[0m     \u001b[39m# Redirect resolving generator.\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     gen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresolve_redirects(r, request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 723\u001b[0m     history \u001b[39m=\u001b[39m [resp \u001b[39mfor\u001b[39;00m resp \u001b[39min\u001b[39;00m gen]\n\u001b[1;32m    724\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    725\u001b[0m     history \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/istatapi/lib/python3.10/site-packages/requests/sessions.py:723\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[39mif\u001b[39;00m allow_redirects:\n\u001b[1;32m    721\u001b[0m     \u001b[39m# Redirect resolving generator.\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     gen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresolve_redirects(r, request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 723\u001b[0m     history \u001b[39m=\u001b[39m [resp \u001b[39mfor\u001b[39;00m resp \u001b[39min\u001b[39;00m gen]\n\u001b[1;32m    724\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    725\u001b[0m     history \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/istatapi/lib/python3.10/site-packages/requests/sessions.py:266\u001b[0m, in \u001b[0;36mSessionRedirectMixin.resolve_redirects\u001b[0;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[39myield\u001b[39;00m req\n\u001b[1;32m    264\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 266\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(\n\u001b[1;32m    267\u001b[0m         req,\n\u001b[1;32m    268\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    269\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    270\u001b[0m         verify\u001b[39m=\u001b[39;49mverify,\n\u001b[1;32m    271\u001b[0m         cert\u001b[39m=\u001b[39;49mcert,\n\u001b[1;32m    272\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    273\u001b[0m         allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    274\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49madapter_kwargs,\n\u001b[1;32m    275\u001b[0m     )\n\u001b[1;32m    277\u001b[0m     extract_cookies_to_jar(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcookies, prepared_request, resp\u001b[39m.\u001b[39mraw)\n\u001b[1;32m    279\u001b[0m     \u001b[39m# extract redirect url, if any, for the next loop\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/istatapi/lib/python3.10/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/envs/istatapi/lib/python3.10/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m         )\n\u001b[1;32m    502\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/istatapi/lib/python3.10/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/istatapi/lib/python3.10/site-packages/urllib3/connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[39m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 386\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[1;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    388\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mconn\u001b[39m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/istatapi/lib/python3.10/site-packages/urllib3/connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[39m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1042\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n\u001b[1;32m   1045\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1046\u001b[0m         (\n\u001b[1;32m   1047\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnverified HTTPS request is being made to host \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1052\u001b[0m         InsecureRequestWarning,\n\u001b[1;32m   1053\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/istatapi/lib/python3.10/site-packages/urllib3/connection.py:414\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    406\u001b[0m     \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mca_certs\n\u001b[1;32m    407\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mca_cert_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(context, \u001b[39m\"\u001b[39m\u001b[39mload_default_certs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    411\u001b[0m ):\n\u001b[1;32m    412\u001b[0m     context\u001b[39m.\u001b[39mload_default_certs()\n\u001b[0;32m--> 414\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m ssl_wrap_socket(\n\u001b[1;32m    415\u001b[0m     sock\u001b[39m=\u001b[39;49mconn,\n\u001b[1;32m    416\u001b[0m     keyfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_file,\n\u001b[1;32m    417\u001b[0m     certfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcert_file,\n\u001b[1;32m    418\u001b[0m     key_password\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_password,\n\u001b[1;32m    419\u001b[0m     ca_certs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_certs,\n\u001b[1;32m    420\u001b[0m     ca_cert_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_dir,\n\u001b[1;32m    421\u001b[0m     ca_cert_data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_data,\n\u001b[1;32m    422\u001b[0m     server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    423\u001b[0m     ssl_context\u001b[39m=\u001b[39;49mcontext,\n\u001b[1;32m    424\u001b[0m     tls_in_tls\u001b[39m=\u001b[39;49mtls_in_tls,\n\u001b[1;32m    425\u001b[0m )\n\u001b[1;32m    427\u001b[0m \u001b[39m# If we're using all defaults and the connection\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[39m# is TLSv1 or TLSv1.1 we throw a DeprecationWarning\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[39m# for the host.\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    431\u001b[0m     default_ssl_context\n\u001b[1;32m    432\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_version \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    433\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock, \u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    434\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock\u001b[39m.\u001b[39mversion() \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mTLSv1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTLSv1.1\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m    435\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/istatapi/lib/python3.10/site-packages/urllib3/util/ssl_.py:449\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    437\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    438\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn HTTPS request has been made, but the SNI (Server Name \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    439\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIndication) extension to TLS is not available on this platform. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    445\u001b[0m         SNIMissingWarning,\n\u001b[1;32m    446\u001b[0m     )\n\u001b[1;32m    448\u001b[0m \u001b[39mif\u001b[39;00m send_sni:\n\u001b[0;32m--> 449\u001b[0m     ssl_sock \u001b[39m=\u001b[39m _ssl_wrap_socket_impl(\n\u001b[1;32m    450\u001b[0m         sock, context, tls_in_tls, server_hostname\u001b[39m=\u001b[39;49mserver_hostname\n\u001b[1;32m    451\u001b[0m     )\n\u001b[1;32m    452\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     ssl_sock \u001b[39m=\u001b[39m _ssl_wrap_socket_impl(sock, context, tls_in_tls)\n",
      "File \u001b[0;32m~/anaconda3/envs/istatapi/lib/python3.10/site-packages/urllib3/util/ssl_.py:493\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[39mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[1;32m    492\u001b[0m \u001b[39mif\u001b[39;00m server_hostname:\n\u001b[0;32m--> 493\u001b[0m     \u001b[39mreturn\u001b[39;00m ssl_context\u001b[39m.\u001b[39;49mwrap_socket(sock, server_hostname\u001b[39m=\u001b[39;49mserver_hostname)\n\u001b[1;32m    494\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    495\u001b[0m     \u001b[39mreturn\u001b[39;00m ssl_context\u001b[39m.\u001b[39mwrap_socket(sock)\n",
      "File \u001b[0;32m~/anaconda3/envs/istatapi/lib/python3.10/ssl.py:513\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_socket\u001b[39m(\u001b[39mself\u001b[39m, sock, server_side\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    508\u001b[0m                 do_handshake_on_connect\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    509\u001b[0m                 suppress_ragged_eofs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    510\u001b[0m                 server_hostname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    511\u001b[0m     \u001b[39m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    512\u001b[0m     \u001b[39m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 513\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msslsocket_class\u001b[39m.\u001b[39;49m_create(\n\u001b[1;32m    514\u001b[0m         sock\u001b[39m=\u001b[39;49msock,\n\u001b[1;32m    515\u001b[0m         server_side\u001b[39m=\u001b[39;49mserver_side,\n\u001b[1;32m    516\u001b[0m         do_handshake_on_connect\u001b[39m=\u001b[39;49mdo_handshake_on_connect,\n\u001b[1;32m    517\u001b[0m         suppress_ragged_eofs\u001b[39m=\u001b[39;49msuppress_ragged_eofs,\n\u001b[1;32m    518\u001b[0m         server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    519\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    520\u001b[0m         session\u001b[39m=\u001b[39;49msession\n\u001b[1;32m    521\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/istatapi/lib/python3.10/ssl.py:1071\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39m==\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[1;32m   1069\u001b[0m             \u001b[39m# non-blocking\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1071\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1072\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1073\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/istatapi/lib/python3.10/ssl.py:1342\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m==\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mand\u001b[39;00m block:\n\u001b[1;32m   1341\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 1342\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1343\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(timeout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ds = DataSet(dataflow_identifier=\"151_914\")\n",
    "test_eq(ds.identifiers['df_id'], '151_914')\n",
    "test_eq(ds.identifiers['df_description'], 'Unemployment  rate')\n",
    "test_eq(ds.identifiers['df_structure_id'], 'DCCV_TAXDISOCCU1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2 = DataSet(dataflow_identifier=\"22_289\")\n",
    "test_eq(ds2.identifiers['df_id'], '22_289')\n",
    "test_eq(ds2.identifiers['df_description'], 'Resident population  on 1st January')\n",
    "test_eq(ds2.identifiers['df_structure_id'], 'DCIS_POPRES1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2.dimensions_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can look at the dimensions of a dataflow by simply accessing its attribute `dimensions`. However, we won't have dimensions' descriptions here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DataSet.dimensions_info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have a look at the dimensions together with their description, we can use the `dimension_info` function. It will return an easy to read pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions_df = ds.dimensions_info()\n",
    "test_eq(dimensions_df.columns, ['dimension', 'dimension_ID', 'description'])\n",
    "dimensions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values that the different dimensions can take can also be explored. The `available_values` attribute contains a dictionary with the dimensions of the dataset as keys. The values of the dictionary are themselves dictionaries which can be accessed through the `values_ids` and `values_description` keys. The former key returns an ID of the dimension's values, the latter a description of these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_dict = ds.available_values\n",
    "test_eq(isinstance(values_dict, dict), True)\n",
    "test_eq(list(values_dict.keys()).sort(), ds.dimensions.sort())\n",
    "test_eq(values_dict['DURATA_DISOCCUPAZ']['values_ids'], ['TOTAL', 'M_GE12'])\n",
    "test_eq(values_dict['DURATA_DISOCCUPAZ']['values_description'], ['total', '12 months and over'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DataSet.get_dimension_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.get_dimension_values('DURATA_DISOCCUPAZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DataSet.set_filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `DataSet.set_filters()` we can filter the dimensions of the dataset by passing the values that we want to filter for. The dataset will then only return data containing our filters. A dictionary with the selected filters is contained in the attribute `DataSet.filters`.\n",
    "\n",
    "**Note** that the arguments of `DataSet.set_filters` are lower case letters, but in `DataSet.filters` they are converted to upper case to be consistent with dimension names on ISTAT API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dz = DataSet(dataflow_identifier=\"139_176\")\n",
    "dz.set_filters(freq=\"M\", tipo_dato=[\"ISAV\", \"ESAV\"], paese_partner=\"WORLD\")\n",
    "\n",
    "test_eq(dz.filters['FREQ'], 'M')\n",
    "test_eq(dz.filters['TIPO_DATO'], [\"ISAV\", \"ESAV\"])\n",
    "test_fail(lambda: dz.filters['freq']) #the filter is not saved in lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
